package main

import (
	"fmt"
	"log"

	"github.com/PuerkitoBio/goquery"
)

func main() {
	// URL of the website to scrape
	url := "https://www.example.com"

	// Use goquery to fetch the HTML of the website
	doc, err := goquery.NewDocument(url)
	if err != nil {
		log.Fatal(err)
	}

	// Use goquery to select and extract data from the website
	doc.Find("div.product").Each(func(i int, s *goquery.Selection) {
		productName := s.Find("h3").Text()
		productPrice := s.Find("span.price").Text()

		fmt.Printf("Product %d: %s - %s\n", i+1, productName, productPrice)
	})
}


'''
To create a web crawler that can traverse a website and extract data from multiple pages and multiple websites using Go, you can take the following steps:

Create a function that takes a URL as an argument and uses goquery to fetch the HTML of the page.
In this function, use goquery to select and extract the data you are interested in, such as links, text, images, etc.
Use the Find method to select all the links in the page and add them to a queue for processing.
Create a loop that repeatedly takes URLs from the queue and calls the function from step 1 with the URL as an argument.
In the function from step 1, you can use a package like "net/url" to extract the domain name of the URL, so you can keep track of which website you are currently visiting and you can stop crawling when you've reached your desired limit.
You could also consider adding additional functionality such as rate limiting to avoid overloading the website with requests, or storing the data in a database.
'''