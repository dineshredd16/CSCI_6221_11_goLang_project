package main

import (
	"fmt"
	"net/http"
	"regexp"
	"strings"

	"github.com/PuerkitoBio/goquery"
)

func main() {
	url := "https://www.cnbc.com/2023/01/20/google-to-lay-off-12000-people-memo-from-ceo-sundar-pichai-says.html?&qsearchterm=google%20layoff"
	res, err := http.Get(url)
	if err != nil {
		fmt.Println("Error fetching the URL:", err)
		return
	}
	defer res.Body.Close()
	if res.StatusCode != 200 {
		fmt.Println("Error: Non-200 status code returned")
		return
	}

	// Load the HTML document
	doc, err := goquery.NewDocumentFromReader(res.Body)
	if err != nil {
		fmt.Println("Error loading the HTML:", err)
		return
	}

	// Find the header text
	header := doc.Find("header h1").Text()
	if header == "" {
		fmt.Println("Header not found on the page")
		return
	}

	// Extract the company name and job cuts
	var companyName string
	var jobCuts int
	if strings.Contains(header, "Google") {
		companyName = "Google"
		if strings.Contains(header, "lay off") {
			// Extract the number of job cuts using a regular expression
			re := regexp.MustCompile("([0-9]+),([0-9]+)")
			n := re.FindString(header)
			fmt.Println("Match:", n)
			n = strings.Replace(n, ",", "", -1)
			fmt.Sscanf(n, "%d", &jobCuts)
		}
	} else {
		fmt.Println("Company not found on the page")
		return
	}

	// Print the results
	fmt.Println("Company Name:", companyName)
	fmt.Println("Number of Job Cuts:", jobCuts)
}



'''
To create a web crawler that can traverse a website and extract data from multiple pages and multiple websites using Go, you can take the following steps:

Create a function that takes a URL as an argument and uses goquery to fetch the HTML of the page.
In this function, use goquery to select and extract the data you are interested in, such as links, text, images, etc.
Use the Find method to select all the links in the page and add them to a queue for processing.
Create a loop that repeatedly takes URLs from the queue and calls the function from step 1 with the URL as an argument.
In the function from step 1, you can use a package like "net/url" to extract the domain name of the URL, so you can keep track of which website you are currently visiting and you can stop crawling when you've reached your desired limit.
You could also consider adding additional functionality such as rate limiting to avoid overloading the website with requests, or storing the data in a database.
'''